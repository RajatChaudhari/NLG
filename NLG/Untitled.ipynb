{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=open(\"../NLG/summary_types.txt\", 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(nltk.word_tokenize(data)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  2081\n",
      "Total Vocab:  2081\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(nltk.word_tokenize(data))\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  1981\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = nltk.word_tokenize(data)[i:i + seq_length]\n",
    "    seq_out = nltk.word_tokenize(data)[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "#X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "X = np.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e76b6a810cff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights-20sep-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1981 samples\n",
      "Epoch 1/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.2888\n",
      "Epoch 00001: loss improved from inf to 4.28824, saving model to weights-20sep-improvement-01-4.2882.hdf5\n",
      "1981/1981 [==============================] - 14s 7ms/sample - loss: 4.2882\n",
      "Epoch 2/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.2817\n",
      "Epoch 00002: loss improved from 4.28824 to 4.28383, saving model to weights-20sep-improvement-02-4.2838.hdf5\n",
      "1981/1981 [==============================] - 13s 7ms/sample - loss: 4.2838\n",
      "Epoch 3/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.2698\n",
      "Epoch 00003: loss improved from 4.28383 to 4.26633, saving model to weights-20sep-improvement-03-4.2663.hdf5\n",
      "1981/1981 [==============================] - 14s 7ms/sample - loss: 4.2663\n",
      "Epoch 4/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.2641\n",
      "Epoch 00004: loss improved from 4.26633 to 4.25934, saving model to weights-20sep-improvement-04-4.2593.hdf5\n",
      "1981/1981 [==============================] - 15s 7ms/sample - loss: 4.2593\n",
      "Epoch 5/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.2613\n",
      "Epoch 00005: loss did not improve from 4.25934\n",
      "1981/1981 [==============================] - 13s 7ms/sample - loss: 4.2609\n",
      "Epoch 6/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.2503\n",
      "Epoch 00006: loss improved from 4.25934 to 4.24842, saving model to weights-20sep-improvement-06-4.2484.hdf5\n",
      "1981/1981 [==============================] - 14s 7ms/sample - loss: 4.2484\n",
      "Epoch 7/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.2524\n",
      "Epoch 00007: loss did not improve from 4.24842\n",
      "1981/1981 [==============================] - 14s 7ms/sample - loss: 4.2510\n",
      "Epoch 8/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.2411\n",
      "Epoch 00008: loss improved from 4.24842 to 4.24020, saving model to weights-20sep-improvement-08-4.2402.hdf5\n",
      "1981/1981 [==============================] - 15s 7ms/sample - loss: 4.2402\n",
      "Epoch 9/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.2372\n",
      "Epoch 00009: loss did not improve from 4.24020\n",
      "1981/1981 [==============================] - 14s 7ms/sample - loss: 4.2412\n",
      "Epoch 10/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.2307\n",
      "Epoch 00010: loss improved from 4.24020 to 4.22949, saving model to weights-20sep-improvement-10-4.2295.hdf5\n",
      "1981/1981 [==============================] - 13s 7ms/sample - loss: 4.2295\n",
      "Epoch 11/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.2344\n",
      "Epoch 00011: loss did not improve from 4.22949\n",
      "1981/1981 [==============================] - 14s 7ms/sample - loss: 4.2357\n",
      "Epoch 12/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.2280\n",
      "Epoch 00012: loss improved from 4.22949 to 4.22537, saving model to weights-20sep-improvement-12-4.2254.hdf5\n",
      "1981/1981 [==============================] - 13s 7ms/sample - loss: 4.2254\n",
      "Epoch 13/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.2120\n",
      "Epoch 00013: loss improved from 4.22537 to 4.21481, saving model to weights-20sep-improvement-13-4.2148.hdf5\n",
      "1981/1981 [==============================] - 14s 7ms/sample - loss: 4.2148\n",
      "Epoch 14/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.2109\n",
      "Epoch 00014: loss improved from 4.21481 to 4.21161, saving model to weights-20sep-improvement-14-4.2116.hdf5\n",
      "1981/1981 [==============================] - 14s 7ms/sample - loss: 4.2116\n",
      "Epoch 15/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.2059\n",
      "Epoch 00015: loss improved from 4.21161 to 4.20384, saving model to weights-20sep-improvement-15-4.2038.hdf5\n",
      "1981/1981 [==============================] - 14s 7ms/sample - loss: 4.2038\n",
      "Epoch 16/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.2007\n",
      "Epoch 00016: loss improved from 4.20384 to 4.19814, saving model to weights-20sep-improvement-16-4.1981.hdf5\n",
      "1981/1981 [==============================] - 14s 7ms/sample - loss: 4.1981\n",
      "Epoch 17/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.1937\n",
      "Epoch 00017: loss improved from 4.19814 to 4.18750, saving model to weights-20sep-improvement-17-4.1875.hdf5\n",
      "1981/1981 [==============================] - 13s 7ms/sample - loss: 4.1875\n",
      "Epoch 18/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.1691\n",
      "Epoch 00018: loss improved from 4.18750 to 4.16820, saving model to weights-20sep-improvement-18-4.1682.hdf5\n",
      "1981/1981 [==============================] - 15s 7ms/sample - loss: 4.1682\n",
      "Epoch 19/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.1495\n",
      "Epoch 00019: loss improved from 4.16820 to 4.15031, saving model to weights-20sep-improvement-19-4.1503.hdf5\n",
      "1981/1981 [==============================] - 14s 7ms/sample - loss: 4.1503\n",
      "Epoch 20/20\n",
      "1950/1981 [============================>.] - ETA: 0s - loss: 4.1323\n",
      "Epoch 00020: loss improved from 4.15031 to 4.12940, saving model to weights-20sep-improvement-20-4.1294.hdf5\n",
      "1981/1981 [==============================] - 13s 7ms/sample - loss: 4.1294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x121db241748>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=50, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../Projects/NLG/Modele200b50.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"weights-improvement-200-0.2683.hdf5\"\n",
    "#filename = \"weights-20sep-improvement-20-4.1294.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.word_tokenize(\"The above graph shows sales history and sales forecast for product PROD_ID at LOC_ID . The sales history is taken into consideration is between SALES_HISTORY_SDATE and SALES_HISTORY_EDATE . As we can see in the graph it attained its peak of PEAK_VAL which was recorded at PEAK_DATE and recorded lowest sales of MINVAL at MINDATE . The average sales during this period was AVG_SALES . The year wise mean for years START_YEAR to END_YEAR are YEALRY_AVG and the yearly trend for year START_YEAR to END_YEAR are TREND . The Finalized planner forecast predicts the future trend for period PFS_YEAR to PFE_YEAR for\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataX[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" sales forecast for product PROD_ID at LOC_ID . The sales history is taken into consideration is between SALES_HISTORY_SDATE and SALES_HISTORY_EDATE . As we can see in the graph it attained its peak of PEAK_VAL which was recorded at PEAK_DATE and recorded lowest sales of MINVAL at MINDATE . The average sales during this period was AVG_SALES . The year wise mean for years START_YEAR to END_YEAR are YEALRY_AVG and the yearly trend for year START_YEAR to END_YEAR are TREND . The Finalized planner forecast predicts the future trend for period PFS_YEAR to PFE_YEAR , it predicts the maximum sales of PMAX_SALES \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_input to have shape (100, 1) but got array with shape (101, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-5538fab9cd8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#print(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint_to_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\D\\Installations\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 821\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    822\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\D\\Installations\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    703\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_or_infer_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     x, _, _ = model._standardize_user_data(\n\u001b[1;32m--> 705\u001b[1;33m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[0;32m    706\u001b[0m     return predict_loop(\n\u001b[0;32m    707\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\D\\Installations\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2426\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2427\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2428\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2430\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\D\\Installations\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    519\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    522\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_input to have shape (100, 1) but got array with shape (101, 1)"
     ]
    }
   ],
   "source": [
    "start = 7#np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ' '.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(200):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    #print(x)\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    #print(result)\n",
    "    #print(i)\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    #sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    #print(int_to_char[i])\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 101, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.word_tokenize(\"above graph shows sales history and sales forecast for product PROD_ID at LOC_ID . The sales history is taken into consideration is between SALES_HISTORY_SDATE and SALES_HISTORY_EDATE . As we can see in the graph it attained its peak of PEAK_VAL which was recorded at PEAK_DATE and recorded lowest sales of MINVAL at MINDATE . The average sales during this period was AVG_SALES . The year wise mean for years START_YEAR to END_YEAR are YEALRY_AVG and the yearly trend for year START_YEAR to END_YEAR are TREND . The Finalized planner forecast predicts the future trend for period PFS_YEAR to PFE_YEAR , it\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
